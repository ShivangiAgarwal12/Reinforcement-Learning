{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A k-armed Bandit Problem\n",
    "by Shivangi Agarwal and Sandeep Banik | Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a k-armed bandit problem, at every time step the user is faced with a choice of picking one action or choice from 'k' possible actions or options. Each action chosen leads to a reward which is governed by an underlying probability distribution. The objective is to maximize the expected total reward acquired over a given possible time period. \n",
    "Conventionally the k armed bandit is viewed as 'k' arms/levers on a single slot machine, and we as the user want to maximize our total reward over given 'N' possible games. The illustration of a k-armed bandit is as shown in the picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RL-k-armed-bandit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the distribution shown is gaussian distribution. However, it can be any general distribution. The slot machine games starts with time = 1, where we play the first game, followed by the second game at time = 2 and so on till time = N. Here, the time instants represent the 'i'th game over N games. We as the user want to ideally pick the arms so as to gain maximum reward (jackpots)over N games played. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RL-k-armed-bandit-N-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action/lever of the possible 'k' levers provide an expected reward. The expected $\\textit{value}$ at the time $t$ of such an action (a) is given by $q_{*}(a)$ defined as,\n",
    "\n",
    "$q_{*}(a) = E[R_{t} | A_{t} = a]$,\n",
    "\n",
    "where $R_{t}$ is the reward at a given time $t$ and $A_{t}$ is the action at the time $t$. However, as shown in the figures above we see that we don't know these distributions and therefore we cannot determine the expected value for a given action $a$.\n",
    "Based on the games being played we can estimate the value of such actions taken which is given by $Q_{t}(a)$, and ideally we want this to be as close to $q_{*}(a)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any given time, if we know the estimates of the set of actions, and if we choose the one which provides us the largest reward (greedy) it is said that we $\\textit{exploit}$ the knowledge of the system. However, if we choose any non-greedy action then we say we explore the system. Non-greedy actions can enable us to explore and achieve a better estimates of all the set of actions. This is illustrated in the figure below. There is a trade-off between exploration and exploitation. Exploitation maybe benifical in the short run, however it might be better to explore in the long run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k-armed-bandit-exploitation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k-armed-bandit-exploration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of methods which use the value of the estimates to make decisions (actions) is collectively called action value methods.\n",
    "The natural way of computing the average value at the time $t$ for a given action $a$ is given as,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Q_{t}(a) = \\frac{\\text{Sum of the rewards when chosen the action a before the time t}}{\\text{Number of times action a is chosen}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "{Q_{t}(a) = \\frac{\\sum_{i=1}^{t-1} R_{i}.I_{A_{t=a}}}{\\sum_{i=1}^{t-1}I_{A_{t=a}}}}\n",
    "\\end{equation}\n",
    "\n",
    "Where $I$ is the indicator variable corresponding to 1 when the action $a$ was chosen and 0 therwise. When the numerator is zero then $Q_{t}$ takes some default value, and when it goes to infinity, $Q_{t}$ converges to $q_{*}$ by law of large numbers.\n",
    "\n",
    "After estimating Q the greedy action is given as,\n",
    "\\begin{equation}\n",
    "A_{t} = arg \\max_{a} \\ Q_{t}(a)\n",
    "\\end{equation}\n",
    "\n",
    "This greedy action exploits the system from the learned estimates. \n",
    "An alternative of the greedy action is to choose a non greedy action once in a while with a probability of $\\epsilonhese methods are called $\\epsilon$-greedy methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import math\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "t = 1000\n",
    "epsilon = 0.1\n",
    "epsilon1 = 0.0\n",
    "\n",
    "bandit_number = 2000\n",
    "\n",
    "q = np.zeros((bandit_number,n))\n",
    "q1 = np.full((bandit_number,n),5)\n",
    "N = np.zeros((bandit_number,n))\n",
    "N1 = np.zeros((bandit_number,n))\n",
    "a_opt = np.zeros((bandit_number,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ec135ab6c78a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0ma_upper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_ucb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmcb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_ucb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_ucb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmcb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mreward_ucb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_upper\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ec135ab6c78a>\u001b[0m in \u001b[0;36mbandit\u001b[1;34m(a, ids)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbandit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbandit_m1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bandit_m1 = np.random.normal(loc=0,scale=1,size=(bandit_number,n))\n",
    "#bandit_m1 = np.random.randint(1,10,size=(n,1))\n",
    "\n",
    "def bandit(a,ids):\n",
    "    \n",
    "    R = np.random.normal(loc=bandit_m1[ids,a],scale=1,size=1)\n",
    "\n",
    "    return R\n",
    "cum_reward = []\n",
    "cum_action  = []\n",
    "\n",
    "opt_act = []\n",
    "\n",
    "\n",
    "for episode in range(1, t):\n",
    "    local_reward = 0.0\n",
    "    action_history = 0.0\n",
    "    upper_rewards = 0.0\n",
    "    for machine in range(1,bandit_number):\n",
    "        #pdb.set_trace()\n",
    "        p = np.random.rand()\n",
    "        if p <= epsilon:\n",
    "            a =  np.random.randint(1,high=n)\n",
    "        else:\n",
    "            a = np.argmax(q[machine,:])\n",
    "        rewards = bandit(a,machine)\n",
    "        \n",
    "        N[machine,a] = N[machine,a] +1\n",
    "        q[machine,a] = q[machine,a] + (1/N[machine,a])*(rewards-q[machine,a])\n",
    "        a_opt[machine,:] = np.argmax(q[machine,:])\n",
    "        \n",
    "        local_reward += rewards\n",
    "        action_history += (q[machine,a])/(np.max(bandit_m1[machine,:])+1)\n",
    "\n",
    "        \n",
    "    cum_reward.append(local_reward/bandit_number)\n",
    "    opt_act.append(action_history/bandit_number)\n",
    "\n",
    "    \n",
    "\n",
    "#%%upper confidence bound action selection\n",
    "c = 2.0\n",
    "a_upper = np.zeros((bandit_number,n))\n",
    "\n",
    "q_ucb = np.zeros((bandit_number,n))\n",
    "N_ucb = np.zeros((bandit_number,n))\n",
    "\n",
    "reward_ucb = []\n",
    "reward_ucb.append(0)\n",
    "\n",
    "cum_reward1 = []\n",
    "upper_action = []\n",
    "\n",
    "for ep_ucb in range(1,t):\n",
    "    lc_reward_ucb = 0.0\n",
    "    opt_a_ucb = 0.0\n",
    "    for mcb in range(1,bandit_number):\n",
    "        \n",
    "        a_upper = np.argmax(q_ucb[mcb,:] + c*(np.sqrt(math.log(ep_ucb+1))/(N_ucb[mcb,:]+1)))\n",
    "        reward_ucb = bandit(a_upper,mcb)\n",
    "        \n",
    "        \n",
    "        N_ucb[mcb,a_upper] = N_ucb[mcb,a_upper] + 1\n",
    "        q_ucb[mcb,a_upper] = q_ucb[mcb,a_upper] + (1/N_ucb[mcb,a_upper])*(reward_ucb-q_ucb[mcb,a_upper])\n",
    "        \n",
    "        \n",
    "        \n",
    "        lc_reward_ucb +=reward_ucb\n",
    "        opt_a_ucb += (q_ucb[mcb,a_upper])/(np.max(bandit_m1[mcb,:])+1)\n",
    "        \n",
    "    cum_reward1.append(lc_reward_ucb/bandit_number)\n",
    "    upper_action.append(opt_a_ucb/bandit_number)\n",
    "    \n",
    "    #reward_ep.append(cum_reward/(t+1))\n",
    "    \n",
    "#%%plot graphs\n",
    "#total_rewards = np.array(total_rewards)/(n*t)\n",
    "#plt.plot(cum_reward,'r',label = 'Epsilon greedy')\n",
    "#plt.plot(cum_reward1,'g',label = 'optimistic')\n",
    "#plt.plot(opt_act)\n",
    "plt.plot(upper_action)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
