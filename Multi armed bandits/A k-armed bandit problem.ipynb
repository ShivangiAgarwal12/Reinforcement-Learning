{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A k-armed Bandit Problem\n",
    "by __Shivangi Agarwal and Sandeep Banik__ | Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a __k-armed bandit__ problem, at every time step the user is faced with a choice of picking one action or choice from $k$ possible actions or options. Each action chosen leads to a reward which is governed by an underlying probability distribution. The objective is to maximize the expected total reward acquired over a given possible time period. \n",
    "Conventionally the k armed bandit is viewed as $k$ arms/levers on a single slot machine, and we as the user want to maximize our total reward over given $N$ possible games. An illustration of a _k-armed bandit_ is as shown in the figure below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RL-k-armed-bandit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_ that the distribution shown is Gaussian distribution. However, it can be any general distribution. The slot machine games starts with $time = 1$, where we play the first game, followed by the second game at $time = 2$ and so on till time = $N$. Here, the time instants represent the $i$th game over $N$ games. We as the user want to ideally pick the arms so as to gain maximum reward (jackpots) over $N$ games played. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RL-k-armed-bandit-N-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action/lever of the possible $k$ levers provide an expected reward. The expected $\\textit{value}$ at the time $t$ of such an action _(a)_ is given by $q_{*}(a)$ defined as,\n",
    "\n",
    "$q_{*}(a) = E[R_{t} | A_{t} = a]$,\n",
    "\n",
    "where $R_{t}$ is the reward at a given time $t$ and $A_{t}$ is the action at the time $t$. However, as shown in the figures above we see that we don't know these distributions and therefore we cannot determine the expected value for a given action $a$.\n",
    "Based on the games being played we can estimate the value of such actions taken which is given by $Q_{t}(a)$, and ideally we want this to be as close to $q_{*}(a)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any given time, if estimates of the set of actions are known, and subsequently we choose the one which provides us with largest reward (greedy) it is said that we have $\\textit{exploited}$ the knowledge of system. However, if any non-greedy action is chosen, we classify it as exploring the system. Non-greedy actions can enable us to explore and achieve a better estimates of all the set of actions. This is illustrated in the figure below. Though there is a trade-off between exploration and exploitation. Exploitation maybe beneficial in the short run, however it might be better to explore in the long run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k-armed-bandit-exploitation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k-armed-bandit-exploration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of methods which use the value of the estimates to make decisions (actions) is collectively called __action value methods__.\n",
    "The natural way of computing the average value at the time $t$ for a given action $a$ is given as,\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{equation}\n",
    "Q_{t}(a) = \\frac{\\text{Sum of the rewards when chosen the action a before the time t}}{\\text{Number of times action a is chosen}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{Q_{t}(a) = \\frac{\\sum_{i=1}^{t-1} R_{i}.I_{A_{t=a}}}{\\sum_{i=1}^{t-1}I_{A_{t=a}}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $I$ is the indicator variable corresponding to 1 when the action $a$ was chosen and 0 therwise. When the numerator is zero then $Q_{t}$ takes some default value, and when it goes to infinity, $Q_{t}$ converges to $q_{*}$ by law of large numbers.\n",
    "\n",
    "After estimating $Q$ the greedy action is given as,\n",
    "$$\n",
    "A_{t} = arg \\max_{a} \\ Q_{t}(a)\n",
    "$$\n",
    "This greedy action exploits the system from learned estimates. \n",
    "An alternative of the greedy action is to choose a random action once in a while with a probability of $\\epsilon$. Such methods are called __$\\epsilon$-greedy__ methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates $Q$ at any given time instant $n$ is given by,\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Q_{n+1} & = \\frac{1}{n}\\sum_{i=1}^{n}R_{i} \\\\\n",
    "& = \\frac{1}{n} \\left \\{ R_{n} + \\sum_{i=1}^{n-1}R_{i} \\right \\} \\\\\n",
    "& = \\frac{1}{n} \\left \\{ R_{n} + (n-1)\\frac{1}{n-1}\\sum_{i=1}^{n-1}R_{i} \\right \\} \\\\\n",
    "& = \\frac{1}{n} \\left \\{ R_{n} + (n-1)Q_{n} \\right \\} \\\\\n",
    "& = \\frac{1}{n} \\left \\{ R_{n} + nQ_{n} - Q_{n} \\right \\} \\\\\n",
    "& = Q_{n} + \\frac{1}{n} \\left \\{ R_{n} - Q_{n} \\right \\} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "The update is simply given as,\n",
    "\n",
    "$\n",
    "\\text{New_Estimate} = \\text{Old_Estimate} + \\text{Step_Size}\\left \\{\\text{Target} - \\text{Old_Estimate} \\right \\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $N(a)$ be the number of times the action $a$ was chosen.\n",
    "\n",
    "Initialization: For all the set of actions i.e., $a=1$ to $k$,\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "Q(a) \\leftarrow 0 \\\\\n",
    "N(a) \\leftarrow 0 \\\\\n",
    "\\text{Loop forever:} \\\\\n",
    "& \\text{action probability} \\leftarrow \\text{sample from [0,1)} \\\\\n",
    "& A = \\begin{cases} arg \\max_{a} Q_{a} \\quad \\text{if action probability} \\geq \\epsilon \\\\ \\text{random action} \\quad \\text{otherwise} \\end{cases} \\\\\n",
    "& R \\leftarrow \\textit{bandit(A)} \\\\\n",
    "& N(A) \\leftarrow N(A) + 1 \\\\\n",
    "& Q(A) \\leftarrow Q(A) + \\frac{1}{n}[R(A) - Q(A)]\\\\\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "Here, a bandit refers to the process/machine which takes in the action and gives the reward. For practial purposes, loop is terminated after convergence is achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 10-armed bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a working code of the above theory, we test the algorithm in a 10 armed bandit condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import math\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "t = 1000\n",
    "epsilon = 0.1\n",
    "epsilon1 = 0.0\n",
    "\n",
    "bandit_number = 20\n",
    "\n",
    "q = np.zeros((bandit_number,n))\n",
    "q1 = np.full((bandit_number,n),5)\n",
    "N = np.zeros((bandit_number,n))\n",
    "N1 = np.zeros((bandit_number,n))\n",
    "a_opt = np.zeros((bandit_number,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_m1 = np.random.normal(loc=0,scale=1,size=(bandit_number,n))\n",
    "#bandit_m1 = np.random.randint(1,10,size=(n,1))\n",
    "\n",
    "def bandit(a,ids):\n",
    "    \n",
    "    R = np.random.normal(loc=bandit_m1[ids,a],scale=1,size=1)\n",
    "\n",
    "    return R\n",
    "\n",
    "cum_reward = []\n",
    "cum_action  = []\n",
    "opt_act = []\n",
    "\n",
    "\n",
    "for episode in range(1, t):\n",
    "    local_reward = 0.0\n",
    "    action_history = 0.0\n",
    "    upper_rewards = 0.0\n",
    "    for machine in range(1,bandit_number):\n",
    "        #pdb.set_trace()\n",
    "        p = np.random.rand()\n",
    "        if p <= epsilon:\n",
    "            a =  np.random.randint(1,high=n)\n",
    "        else:\n",
    "            a = np.argmax(q[machine,:])\n",
    "        rewards = bandit(a,machine)\n",
    "        \n",
    "        N[machine,a] = N[machine,a] +1\n",
    "        q[machine,a] = q[machine,a] + (1/N[machine,a])*(rewards-q[machine,a])\n",
    "        a_opt[machine,:] = np.argmax(q[machine,:])\n",
    "        \n",
    "        local_reward += rewards\n",
    "        action_history += (q[machine,a])/(np.max(bandit_m1[machine,:])+1)\n",
    "\n",
    "        \n",
    "    cum_reward.append(local_reward/bandit_number)\n",
    "    opt_act.append(action_history/bandit_number)\n",
    "\n",
    "    \n",
    "\n",
    "#%%upper confidence bound action selection\n",
    "c = 2.0\n",
    "a_upper = np.zeros((bandit_number,n))\n",
    "\n",
    "q_ucb = np.zeros((bandit_number,n))\n",
    "N_ucb = np.zeros((bandit_number,n))\n",
    "\n",
    "reward_ucb = []\n",
    "reward_ucb.append(0)\n",
    "\n",
    "cum_reward1 = []\n",
    "upper_action = []\n",
    "\n",
    "for ep_ucb in range(1,t):\n",
    "    lc_reward_ucb = 0.0\n",
    "    opt_a_ucb = 0.0\n",
    "    for mcb in range(1,bandit_number):\n",
    "        \n",
    "        a_upper = np.argmax(q_ucb[mcb,:] + c*(np.sqrt(math.log(ep_ucb+1))/(N_ucb[mcb,:]+1)))\n",
    "        reward_ucb = bandit(a_upper,mcb)\n",
    "        \n",
    "        \n",
    "        N_ucb[mcb,a_upper] = N_ucb[mcb,a_upper] + 1\n",
    "        q_ucb[mcb,a_upper] = q_ucb[mcb,a_upper] + (1/N_ucb[mcb,a_upper])*(reward_ucb-q_ucb[mcb,a_upper])\n",
    "        \n",
    "        \n",
    "        \n",
    "        lc_reward_ucb +=reward_ucb\n",
    "        opt_a_ucb += (q_ucb[mcb,a_upper])/(np.max(bandit_m1[mcb,:])+1)\n",
    "        \n",
    "    cum_reward1.append(lc_reward_ucb/bandit_number)\n",
    "    upper_action.append(opt_a_ucb/bandit_number)\n",
    "    \n",
    "    #reward_ep.append(cum_reward/(t+1))\n",
    "    \n",
    "#%%plot graphs\n",
    "#total_rewards = np.array(total_rewards)/(n*t)\n",
    "#plt.plot(cum_reward,'r',label = 'Epsilon greedy')\n",
    "#plt.plot(cum_reward1,'g',label = 'optimistic')\n",
    "#plt.plot(opt_act)\n",
    "plt.plot(upper_action)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
