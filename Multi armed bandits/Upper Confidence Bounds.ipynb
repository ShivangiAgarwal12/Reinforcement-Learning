{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Bounds (A k-armed bandit problem)\n",
    "by __Shivangi Agarwal and Sandeep Banik__ | Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon greedy provides a method to choose possible actions encouraging exploration. However while choosing the actions at any given time, the method is unbiased and chooses any action with equal probability. One would expect that if an arm/action is choosen multiple times, its estimate is much better as compared to other arms/actions. This is exactly captured by the upper confidence bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/UCB-intro.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/UCB-define.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation to choose an action is given by, \n",
    "\n",
    "$A_{t} = arg \\max_{a} \\left [ Q_{t}(a) + c\\sqrt{\\frac{ln(t)}{N_{t}(a)}} \\right ] $\n",
    "\n",
    "$N_t(a)$ is number of times the action $a$ is choosen, which reduces the uncertainity as is present in the denominator. The numerator term, $ln(t)$ keeps increasing with each time step. However, the increase is logarithmic in nature, meaning its influence decreases with each time but is unbounded. $c$ represent the confidence level. We determine the action over,\n",
    "\n",
    "$ \\text{Action}(t)  = arg \\max_{a} \\left [ \\text{Estimate}(t) + \\text{upper bound} \\right ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/UCB-insight.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Initialisations\n",
    "#total number of bandit arms\n",
    "n = 10\n",
    "\n",
    "#total number of experiments\n",
    "t = 1000\n",
    "\n",
    "#probability of random exploration(fraction)\n",
    "epsilon = []\n",
    "epsilon = [2.0,0.1]\n",
    "\n",
    "#number of bandits(episodes)\n",
    "bandit_number = 2000\n",
    "\n",
    "\n",
    "#optimal action that should be chosen for each bandit\n",
    "a_opt = np.zeros((bandit_number,n))\n",
    "\n",
    "#initial same values to all the arms\n",
    "Q_star = np.random.normal(loc=0,scale=1,size=(bandit_number,n))\n",
    "\n",
    "#cumulative reward at each episode for chossing an action by probability epsilon\n",
    "avg_reward = np.asmatrix(np.zeros((len(epsilon),t)))\n",
    "\n",
    "#function Bandit taking action and bandit as input, returning reward.\n",
    "def bandit(a,ids):\n",
    "    R = np.random.normal(loc=Q_star[ids,a],scale=1,size=1)\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%A simple bandit algorithm:crappy cleaning\n",
    "for k in range(len(epsilon)):\n",
    "    N = np.zeros((bandit_number,n))\n",
    "    Q = np.zeros((bandit_number,n))\n",
    "    for episode in range(1, t):\n",
    "        local_reward = 0.0\n",
    "        action_history = 0.0\n",
    "        upper_rewards = 0.0\n",
    "        for machine in range(1,bandit_number):\n",
    "            if (epsilon[k] == 2.0):\n",
    "                a = np.argmax(Q[machine,:] + epsilon[k]*(np.sqrt(math.log(episode+1))/(N[machine,:]+1)))\n",
    "            else:\n",
    "                p = np.random.rand()\n",
    "                if (p <= epsilon[k]):\n",
    "                    a = np.random.randint(1,high = n)\n",
    "                else:\n",
    "                    a = np.argmax(Q[machine,:])\n",
    "\n",
    "            rewards = bandit(a,machine)\n",
    "            \n",
    "            N[machine,a] = N[machine,a] + 1\n",
    "\n",
    "            #update the estimate\n",
    "            Q[machine,a] = Q[machine,a] + (1/N[machine,a])*(rewards-Q[machine,a])\n",
    "            \n",
    "            local_reward += rewards\n",
    "            \n",
    "            idx = np.argmax(Q_star[machine,:])\n",
    "            action_history += (a ==  idx)\n",
    "        \n",
    "        avg_reward[k,episode] = (local_reward/bandit_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
